{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spotify Data Pipeline Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will guide you through the different steps to successfully complete a Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Py Libraries Installation and Initialising the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages.\n",
    "!pip install -q findspark  # Used to locate Spark in the environment\n",
    "!pip install py4j          # Enables Python to communicate with the JVM (required for PySpark)\n",
    "!pip install plotly         # For interactive visualizations\n",
    "!pip install google-generativeai  # For using Gemini (or other generative AI models)\n",
    "\n",
    "!pip install prefect       # For workflow orchestration\n",
    "!pip install typing_extensions  # For type hinting\n",
    "\n",
    "# Installing and setting up MySQL\n",
    "!pip -q install PyMySQL    # Python driver for MySQL\n",
    "!pip install prettytable   # For displaying data in tables\n",
    "!pip install sqlalchemy    # Object-relational mapper (ORM) for database interaction\n",
    "!pip install ipython-sql   # For running SQL queries in Jupyter notebooks\n",
    "\n",
    "# Load the SQL magic extension for Jupyter notebooks\n",
    "%load_ext sql\n",
    "%config SqlMagic.style = '_DEPRECATED_DEFAULT'  # Set the style for displaying SQL results\n",
    "\n",
    "# Connect to MySQL database running in a Docker container\n",
    "%sql mysql+pymysql://root:root@host.docker.internal:8083\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import findspark\n",
    "findspark.init()  # Initialize findspark to locate Spark\n",
    "findspark.find()  # Find the Spark installation\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from prefect import task, flow  # For creating Prefect tasks and flows\n",
    "from typing import List\n",
    "import pyspark.sql.types as T  # For Spark data types\n",
    "import pyspark.sql.functions as F  # For Spark SQL functions\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"MWC Data Analytics Lab\") \\\n",
    "        .config(\"spark.jars\", \"./pkgs/mysql-connector-j-9.2.0.jar\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark  # Display the SparkSession object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "id": "mlFk8NEA3Yca",
    "outputId": "e7967644-2810-43b2-c498-8ce3a7dc306d"
   },
   "outputs": [],
   "source": [
    "%sql SHOW DATABASES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Database for the Workshop\n",
    "We will be creating tables throughout the Workshop and will need a Database to store them. Create a database in MySql using the Magic command \"%sql\" at the beginning of the line.\n",
    "\n",
    "Name of the Database: \"mwc_lab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql CREATE DATABASE IF NOT EXISTS mwc_lab;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcbHCrCivMpO"
   },
   "source": [
    "# Creating our Secrets -- Spotify and Gemini\n",
    "### Getting Started with Spotify's Web API\n",
    "> https://developer.spotify.com/documentation/web-api/tutorials/getting-started\n",
    "\n",
    "* App Name: Up to you, not relevant. Can use \"MWC\"\n",
    "* App Description: Up to you, not relevant. Can use \"End-to-end Data Analytics Workshop\"\n",
    "* Redirect URIs: http://localhost:8888/callback\n",
    "* APIs Used: Web API\n",
    "\n",
    "Once you have completed all the steps, you should have access to your Client ID and Client Secret. Copy them in the cell below. \n",
    "\n",
    "### Getting Started with Gemini\n",
    "\n",
    "> https://pypi.org/project/google-generativeai/ \n",
    "\n",
    "Only the first three steps are needed, **We just need to create an API Key**.\n",
    "Once you have the API Key, copy it in the cell below \"gemini_api_key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = \"\"   # Introduce your Spotify's Client ID\n",
    "client_secret = \"\" # Introduce your Spotify's Client Secret\n",
    "gemini_api_key = \"\" # Introduce your Gemini API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **BIG DISCLAIMER: Storing such information in python variables IS NOT a good practice. In order to remove a bit of complexity from the workshop, we are proceeding this way. Please DO NOT SHARE these.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Spotify API Connector\n",
    "\n",
    "We will need several interactions with the API, so we will build our own python connector to interact with the several entrypoints that the API provides. \n",
    "\n",
    "There is an already built-in library for this, SpotiPy (https://spotipy.readthedocs.io/en/2.22.1/). \n",
    "However, for the sake of making the workshop a bit more interesting, let's build our own to demystify the complexity of working with APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame, Column\n",
    "from typing import List\n",
    "from datetime import date\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Get the Spark Context from the existing Spark Session\n",
    "sc = spark.sparkContext\n",
    "\n",
    "class SpotifyAPI:\n",
    "    \"\"\"\n",
    "    A class to interact with the Spotify API.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> object:\n",
    "        \"\"\"\n",
    "        Initializes the SpotifyAPI object with client credentials and base URLs.\n",
    "        \"\"\"\n",
    "        self.client_id = client_id  # Assuming client_id is defined elsewhere\n",
    "        self.client_secret = client_secret  # Assuming client_secret is defined elsewhere\n",
    "        self.base_url = \"https://api.spotify.com/v1\"  # Spotify API base URL\n",
    "        self.token_url = \"https://accounts.spotify.com/api/token\"  # Spotify token URL\n",
    "        self.access_token = self.get_access_token()  # Get the access token\n",
    "\n",
    "    def get_access_token(self) -> str:\n",
    "        \"\"\"\n",
    "        Retrieves an access token from the Spotify API using client credentials.\n",
    "        \"\"\"\n",
    "        response = requests.post(\n",
    "            self.token_url,\n",
    "            data={\"grant_type\": \"client_credentials\"},\n",
    "            auth=(self.client_id, self.client_secret)\n",
    "        )\n",
    "        return response.json()[\"access_token\"]\n",
    "\n",
    "    def parse_json(self, endpoint: str, object_id: str = None, params: dict = {}) -> str:\n",
    "        \"\"\"\n",
    "        Sends a GET request to the Spotify API and returns the JSON response.\n",
    "        \"\"\"\n",
    "        url = f'{self.base_url}/{endpoint}/'\n",
    "\n",
    "        if object_id:\n",
    "            url += f\"{object_id}\"\n",
    "\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                url,\n",
    "                headers=dict(Authorization=f\"Bearer {self.access_token}\"),\n",
    "                params=params\n",
    "            )\n",
    "            response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching data from {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def read_json_to_df(obj) -> object:\n",
    "        \"\"\"\n",
    "        Converts a JSON object to a Spark DataFrame.\n",
    "        \"\"\"\n",
    "        return spark.read.json(sc.parallelize([json.dumps(obj)]))\n",
    "\n",
    "# Create an instance of the SpotifyAPI class\n",
    "sp = SpotifyAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefect -- Orchestrating tasks and Flows\n",
    "Prefect is a **modern data workflow orchestration tool designed to help you build, run, and monitor robust and reliable data pipelines**. It provides a flexible and intuitive way to define complex workflows as code, making it easier to manage dependencies, handle errors, and observe the execution of your data processes.\n",
    "\n",
    "Brief Introduction to Flows and Tasks:\n",
    "\n",
    "Flows:\n",
    "* A flow is the fundamental unit of a Prefect workflow. It represents the overall sequence of operations that make up your data pipeline.\n",
    "* Flows are defined using the @flow decorator.\n",
    "* They define the overall structure and dependencies of your workflow.\n",
    "* Flows orchestrate the execution of tasks.\n",
    "  \n",
    "Tasks:\n",
    "* A task is a discrete unit of work within a flow. It represents a single operation, such as reading data from a database, transforming data, or writing data to a file.\n",
    "* Tasks are defined using the @task decorator.\n",
    "* They encapsulate the logic for performing a specific operation.\n",
    "* Tasks can have dependencies on other tasks.   \n",
    "* Tasks can return values that can be used by other tasks in the flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">We will be running all our tasks on a temp server that is set up automatically for us for simplicity of the environment setup. However, we could do flows and deployments into a separate server and see them through their UI or add schedules \n",
    "\n",
    ">You can check how the UI looks like in a self-hosted Prefect server at http://0.0.0.0:4200/dashboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jsSvrSNijQ8o",
    "outputId": "12c58bd2-1b0b-45b6-e7d9-f75b9cdae205"
   },
   "outputs": [],
   "source": [
    "from prefect.cache_policies import NO_CACHE\n",
    "\n",
    "\n",
    "@task\n",
    "def get_api_response(endpoint: str, details: str = None):\n",
    "    try:\n",
    "        response = sp.parse_json(endpoint, details)\n",
    "    except Exception as e:\n",
    "        raise(e)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "@task(cache_policy=NO_CACHE)\n",
    "def write_df(df: DataFrame, target_db: str, table_name: str) -> None: \n",
    "    connection_properties = {\n",
    "    \"user\" : \"root\",\n",
    "    \"passw\" : \"root\",\n",
    "    \"driver\" : \"com.mysql.jdbc.Driver\",\n",
    "    \"base_url\" : \"jdbc:mysql://host.docker.internal:8083/\",\n",
    "    \"db\" : f\"{target_db}\"\n",
    "    }\n",
    "\n",
    "    df.write \\\n",
    "      .format(\"jdbc\") \\\n",
    "      .option(\"driver\",connection_properties[\"driver\"]) \\\n",
    "      .option(\"url\", connection_properties[\"base_url\"] + connection_properties[\"db\"]) \\\n",
    "      .option(\"dbtable\", f\"{table_name}\") \\\n",
    "      .option(\"user\", connection_properties[\"user\"]) \\\n",
    "      .option(\"password\", connection_properties[\"passw\"]) \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .save()\n",
    "\n",
    "\n",
    "@task\n",
    "def read_mysql_table(db: str, table_name: str): \n",
    "    connection_properties = {\n",
    "    \"user\" : \"root\",\n",
    "    \"passw\" : \"root\",\n",
    "    \"driver\" : \"com.mysql.jdbc.Driver\",\n",
    "    \"base_url\" : \"jdbc:mysql://host.docker.internal:8083/\",\n",
    "    \"db\" : f\"{db}\"\n",
    "    }\n",
    "    \n",
    "    df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\",connection_properties[\"driver\"]) \\\n",
    "    .option(\"url\", connection_properties[\"base_url\"] + connection_properties[\"db\"]) \\\n",
    "    .option(\"dbtable\", f\"{table_name}\") \\\n",
    "    .option(\"user\", connection_properties[\"user\"]) \\\n",
    "    .option(\"password\", connection_properties[\"passw\"]) \\\n",
    "    .load()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first Table -- playlist_tracks\n",
    "\n",
    "As a first step, let's create a first table that will hold the cleansed and formatted information extracted from our Playlist. \n",
    "Steps: \n",
    "* Define the playlist we will be working with\n",
    "* Call the function get_playlist_tracks_api_call to get the data\n",
    "* Reformat the spark DataFrame to have the following schema:\n",
    "```bash\n",
    "root\n",
    " |-- track_id: string (nullable = true)\n",
    " |-- artist_id: string (nullable = false)\n",
    " |-- album_id: string (nullable = true)\n",
    " |-- track_name: string (nullable = true)\n",
    " |-- track_popularity: long (nullable = true)\n",
    " |-- track_duration_ms: long (nullable = true)\n",
    " |-- album_release_date: string (nullable = true)\n",
    " |-- track_uri: string (nullable = true)\n",
    " |-- added_at: string (nullable = true)\n",
    "```\n",
    "\n",
    "HINT: There are nested fields in the response, you may need to access Array Structs. You can do so by simply using dots to access different items within an array of fields. \n",
    "E.g.:\n",
    "```python\n",
    "df.printSchema() # To know what schema we are working with\n",
    "\n",
    "...\n",
    ".select(F.col(\"item.subitem\")) # To access nested fields\n",
    "```\n",
    "\n",
    "You can do all this in the empty function get_playlist_tracks_df in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_id = \"167pv4bC72ZabUAzJ6tFyq\" # You can use any playlist id that is public and made by an user. You can use this one by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def get_playlist_tracks_api_call(playlist_id: str):\n",
    "    \"\"\"\n",
    "    Retrieves tracks data for a given playlist ID from the Spotify API.\n",
    "\n",
    "    Args:\n",
    "        playlist_id: The ID of the Spotify playlist.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame containing the raw API response for the playlist tracks.\n",
    "    \"\"\"\n",
    "    playlist_response = get_api_response(\"playlists\", f\"{playlist_id}/tracks\")\n",
    "    return sp.read_json_to_df(playlist_response[\"items\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To check whether your credentials work, call the function get_playlist_tracks_api_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE COMPLETED\n",
    "df = # ...\n",
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cpxg4bVSGhwD",
    "outputId": "33feda8b-0149-4e43-b571-2ad1f62c6bf6"
   },
   "outputs": [],
   "source": [
    "@task\n",
    "def get_playlist_tracks_df(playlist_id: str) -> DataFrame:\n",
    "    \"\"\" TO BE COMPLETED!!!\n",
    "    Processes the raw API response for playlist tracks and extracts relevant information into a structured DataFrame.\n",
    "\n",
    "    Args:\n",
    "        playlist_id: The ID of the Spotify playlist.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame containing the extracted track information.\n",
    "    \"\"\"\n",
    "    df = get_playlist_tracks_api_call(playlist_id) # Check schema of this df to know what we are working with\n",
    "    playlist_tracks_df = (\n",
    "        df\n",
    "        .withColumn(\"artist_id_conc\", F.concat_ws(\",\", \"track.album.artists.id\"))\n",
    "        .withColumn(\"artist_id\", F.split(\"artist_id_conc\", \",\")[0])\n",
    "        .select(\n",
    "            F.col(\"track.id\").alias(\"track_id\"),\n",
    "            F.col(\"artist_id\"),\n",
    "            # TO BE COMPLETED \n",
    "            # ...\n",
    "            # ...\n",
    "    ))\n",
    "    return playlist_tracks_df\n",
    "\n",
    "\n",
    "\n",
    "playlist_tracks_df = get_playlist_tracks_df(playlist_id)\n",
    "playlist_tracks_df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's write this information into a table\n",
    "\n",
    "For this purpose, we have already defined a function to write to our MySQL instance. We just need to call it with the params we want.\n",
    "Function: write_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_df(\n",
    "# TO BE COMPLETED\n",
    "# ...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data in MySql to double-check the write operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether the table has been written and it has data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving forward, we will work with a smaller subset of Data for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose our top 30 tracks based on popularity (top 30 most popular tracks in our playlist).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_tracks_top30 = # TO BE COMPLETED!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief analysis over playlist_tracks top 30\n",
    "### Let's Visualise popularity against Release Year over our top 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = playlist_tracks_top30.withColumn(\"release_year\", F.year(F.to_date(F.col(\"album_release_date\"))))\n",
    "pandas_df = df.select(\"track_popularity\", \"release_year\").toPandas()\n",
    "\n",
    "# Scatter Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x=\"release_year\", y=\"track_popularity\", data=pandas_df)\n",
    "plt.title(\"Track Popularity vs. Album Release Year\")\n",
    "plt.xlabel(\"Album Release Year\")\n",
    "plt.ylabel(\"Track Popularity\")\n",
    "plt.savefig(\"track_popularity_vs_release_year.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plot (Alternative)\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.boxplot(x=\"release_year\", y=\"track_popularity\", data=pandas_df)\n",
    "plt.title(\"Track Popularity Distribution by Album Release Year\")\n",
    "plt.xlabel(\"Album Release Year\")\n",
    "plt.ylabel(\"Track Popularity\")\n",
    "plt.xticks(rotation=45) #rotate x axis labels\n",
    "plt.savefig(\"track_popularity_dist_by_release_year.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Table -- artist_albums\n",
    "\n",
    "Now, let's go into the second step which will be get the different albums per artist.\n",
    "* Complete the code for the function get_artist_albums_df\n",
    "    * Get a list of artist ids with no duplicates -- Select distinct artist ids\n",
    "    * Call the function that triggers the API call -- get_artists_albums\n",
    "    * Format the output dataframe to follow the schema defined below:\n",
    "\n",
    "```bash\n",
    "root\n",
    " |-- artist_name: string (nullable = false)\n",
    " |-- name: string (nullable = true)\n",
    " |-- id: string (nullable = true)\n",
    " |-- release_date: string (nullable = true)\n",
    " |-- total_tracks: long (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats_over_col(col_name: Column, df: DataFrame): \n",
    "    avg = df.select(F.avg(col_name)).collect()[0][0]\n",
    "    stddev = df.select(F.stddev(col_name)).collect()[0][0]\n",
    "    return round(avg), round(stddev)\n",
    "\n",
    "avg_popularity, stddev_popularity = get_stats_over_col(col_name = F.col(\"track_popularity\"), df = playlist_tracks_top30)\n",
    "avg_release_date, stddev_release_date = get_stats_over_col(col_name = F.year(F.col(\"album_release_date\")), df = playlist_tracks_top30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The next retrieval operation may take a few minutes since we need to do several API calls, one per artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "@task(cache_policy=NO_CACHE)\n",
    "def get_artists_albums(artist_ids: List):\n",
    "    \"\"\"\n",
    "    Retrieves and combines album data for a list of artist IDs.\n",
    "\n",
    "    Args:\n",
    "        artist_ids (list): A list of artist IDs (strings or integers).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A single DataFrame containing combined album information for all artists.\n",
    "    \"\"\"\n",
    "    ls_dfs=[]\n",
    "    for i in range(len(artist_ids)):\n",
    "        artist_albums = get_api_response(\"artists\", f\"{artist_ids[i]}/albums\")\n",
    "        ls_dfs += [sp.read_json_to_df(artist_albums[\"items\"])]\n",
    "\n",
    "\n",
    "    return reduce(DataFrame.unionAll, ls_dfs)\n",
    "\n",
    "\n",
    "@task(cache_policy=NO_CACHE)\n",
    "def get_artist_albums_df(df: DataFrame):\n",
    "    \"\"\"\n",
    "    Retrieves and transforms album data for artists present in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): A DataFrame containing artist IDs.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing enriched and transformed album information.\n",
    "    \"\"\"\n",
    "    artists_list = [row.artist_id for row in # {TO BE COMPLETED!!!}.collect()]  \n",
    "    artist_albums_raw =  # TO BE COMPLETED!!! -- Call to the previous function\n",
    "    artist_albums_df = (\n",
    "        artist_albums_raw.withColumn(\"artist_name\", F.concat_ws(\", \", \"artists.name\"))\n",
    "        .select(\n",
    "            F.col(\"artist_name\"),\n",
    "            # TO BE COMPLETED \n",
    "            # ...\n",
    "            # ...\n",
    "\n",
    "        )\n",
    "    )\n",
    "    return artist_albums_df\n",
    "\n",
    "\n",
    "artist_albums_df = get_artist_albums_df(playlist_tracks_top30)\n",
    "artist_albums_df.printSchema()\n",
    "artist_albums_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_albums_df.select(F.col(\"artist_name\"),F.col(\"name\"), F.col(\"id\")).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_df(\n",
    "# TO BE COMPLETED \n",
    "# ...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data in MySql to double-check the write operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third and last table -- album_tracks\n",
    "\n",
    "Similarly to how we did our second table, we now need to get the tracks from each album based on album_ids\n",
    "\n",
    "* Complete the get_album_tracks function\n",
    "    * Calculate duration_minutes col\n",
    "    * Format the dataframe so it follows the schema below: \n",
    "\n",
    "```bash\n",
    "root\n",
    " |-- track_id: string (nullable = true)\n",
    " |-- album_id: string (nullable = false)\n",
    " |-- track_name: string (nullable = true)\n",
    " |-- album_name: string (nullable = false)\n",
    " |-- artist_name: string (nullable = false)\n",
    " |-- duration_minutes: double (nullable = true)\n",
    " |-- explicit: boolean (nullable = true)\n",
    " |-- track_number: long (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(cache_policy=NO_CACHE)\n",
    "def get_album_tracks(album_ids: List):\n",
    "    \"\"\"\n",
    "    Retrieves and combines track data for a list of album IDs.\n",
    "\n",
    "    Args:\n",
    "        album_ids (List): A list of tuples, where each tuple contains (album_id, album_name).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A single DataFrame containing combined track information for all albums.\n",
    "    \"\"\"\n",
    "    ls_dfs=[]\n",
    "    for i in range(len(album_ids)):\n",
    "        album_tracks = get_api_response(\"albums\", f\"{album_ids[i][0]}/tracks\")\n",
    "        album_tracks_df = sp.read_json_to_df(album_tracks[\"items\"])\n",
    "        enriched = album_tracks_df.withColumn(\"album_id\", F.lit(f\"{album_ids[i][0]}\")).withColumn(\"album_name\", F.lit(f\"{album_ids[i][1]}\"))\n",
    "        ls_dfs += [enriched]\n",
    "\n",
    "    return reduce(DataFrame.unionAll, ls_dfs)\n",
    "\n",
    "\n",
    "@task(cache_policy=NO_CACHE)\n",
    "def get_album_tracks_df(df: DataFrame):\n",
    "    \"\"\"\n",
    "    Retrieves and transforms track data for albums present in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): A DataFrame containing artist albums.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing enriched and transformed track information.\n",
    "    \"\"\"\n",
    "    album_ids = [(row.id, row.name) for row in df.select(F.col(\"id\"), F.col(\"name\")).distinct().collect()]\n",
    "    album_tracks_raw = get_album_tracks(album_ids)\n",
    "\n",
    "    album_tracks_df = (\n",
    "        album_tracks_raw\n",
    "        .withColumn(\"artist_name\", F.concat_ws(\", \", \"artists.name\"))\n",
    "        .withColumn(\"duration_minutes\", #TO BE COMPLETED)\n",
    "        .select(\n",
    "            F.col(\"id\").alias(\"track_id\"),\n",
    "            # TO BE COMPLETED \n",
    "            # ...\n",
    "            # ...\n",
    "\n",
    "        )\n",
    "    )\n",
    "    return album_tracks_df\n",
    "    \n",
    "\n",
    "album_tracks_df = get_album_tracks_df(artist_albums_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_tracks_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's write this information into a table\n",
    "\n",
    "For this purpose, we have already defined a function to write to our MySQL instance. We just need to call it with the params we want.\n",
    "Function: write_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "psFTshalEEt4",
    "outputId": "4003a1f8-f64c-4338-dcc0-a98902811d41"
   },
   "outputs": [],
   "source": [
    "# TO BE COMPLETED "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data in MySql to double-check the write operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVSoIKUKEMS4"
   },
   "outputs": [],
   "source": [
    "%sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7v8gP9BhEIl4"
   },
   "outputs": [],
   "source": [
    "%sql ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis over Album Tracks\n",
    "\n",
    "Now we finally got to the data we were interested in -- All the albums and their songs from the most popular artists in one of our playlists.\n",
    "We could elaborate a report to then check which songs from all these, could be potentially interesting to us or use this as a base training for a personal recommendation algorithm.\n",
    "\n",
    "Let's deep dive into this dataset and generate some visualisations to digest the information better. \n",
    "\n",
    "To visualise data in a notebook, we will need to convert from Pyspark DataFrames to Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas DataFrame\n",
    "pandas_df = album_tracks_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Track Duration Distribution (Histogram or KDE Plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(pandas_df['duration_minutes'], bins=30, kde=True)\n",
    "plt.title(\"Distribution of Track Duration (Minutes)\")\n",
    "plt.xlabel(\"Track Duration (Minutes)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"track_duration_distribution.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicit Content Distribution (Count Plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='explicit', data=pandas_df)\n",
    "plt.title(\"Distribution of Explicit Content\")\n",
    "plt.xlabel(\"Explicit\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.savefig(\"distribution_of_explicit_content.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of Track Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(pandas_df['track_number'], bins=20, kde=False)\n",
    "plt.title(\"Distribution of Track Number\")\n",
    "plt.xlabel(\"Track Number\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"distribution_of_track_number.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Artist Track Count (Bar Plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_counts = pandas_df['artist_name'].value_counts().head(10) #Top 10 artists.\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=artist_counts.index, y=artist_counts.values)\n",
    "plt.title(\"Top 10 Artists Track Count\")\n",
    "plt.xlabel(\"Artist Name\")\n",
    "plt.ylabel(\"Track Count\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"top_10_artist_track_count.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Track Duration vs. Track Number (Scatter plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='track_number', y='duration_minutes', data=pandas_df)\n",
    "plt.title(\"Track Duration vs. Track Number\")\n",
    "plt.xlabel(\"Track Number\")\n",
    "plt.ylabel(\"Track Duration (Minutes)\")\n",
    "plt.savefig(\"track_duration_vs_track_number.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Gemini Analysis Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leveraging googles open api call to Gemini, we can make requests to their LLM and obtain the responses. \n",
    "This allows us to seamlessly interact with public genAI solutions and gives us tons of opportunities to automate actions and jump straight into helpful insights. \n",
    "\n",
    "For that, we just need to initialise a client with the key we got at the beginning of the workshop, define our prompt with all the material we want to include for context enriching, and that's it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import google.generativeai as genai\n",
    "import PIL.Image\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
    "\n",
    "genai.configure(api_key=gemini_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Storing the plot images from the previous exercises into python variables\n",
    "img1 = PIL.Image.open(\"track_duration_vs_track_number.png\")\n",
    "# TO BE COMPLETED\n",
    "# ...\n",
    "\n",
    "\n",
    "# Initialising the client and preparing our context\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "response = model.generate_content(img1)\n",
    "# TO BE COMPLETED\n",
    "# ...\n",
    "\n",
    "# Prompt definition\n",
    "prompt = \"\"\" Create a detailed report over the track and album data extracted from Spotify. \n",
    "Create summary points per each image, explaining in simple terms what each plot represents.\n",
    "Provide key findings and actionable insights based on the data extracted from these plots.\n",
    "\"\"\"\n",
    "\n",
    "# Response \n",
    "response = model.generate_content([prompt, img1, img2, img3, img4, img5, img6, img7], stream=True)\n",
    "response.resolve()\n",
    "formatted_text = to_markdown(response.text)\n",
    "display(formatted_text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
